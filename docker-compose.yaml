version: '3.7'

# --------------- AIRFLOW ENVIRONMENT VARIABLES ---------------
x-environment: &airflow_environment
  - AIRFLOW__CORE__EXECUTOR=LocalExecutor
  - AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@airflowdb:5432/airflowdb
  - AIRFLOW__CORE__FERNET_KEY=YlCImzjge_TeZc7jPJ7Jz2pgOtb4yTssA1pVyqIADWg=
  - AIRFLOW__DATABASE__LOAD_DEFAULT_CONNECTIONS=False
  - AIRFLOW__CORE__STORE_SERIALIZED_DAGS=True
  - AIRFLOW__CORE__LOAD_EXAMPLES=False
  - AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION=True
  - AIRFLOW__CORE__DEFAULT_TIMEZONE=America/Bogota

x-airflow-image: &airflow_image apache/airflow:2.3.3

# --------------- NETWORKS -------------------------------
networks:
  airflow_spark:
    driver: bridge

# --------------- SERVICES -------------------------------


services:
  airflowdb:
    image: postgres 
    container_name: airflowdb
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflowdb
    ports:
      - "5432:5432"
    networks:
      - airflow_spark
    volumes:
      - postgres-airflowdb-volume:/var/lib/postgresql
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always
  
  airflow_init:
    build:
      context: docker
      dockerfile: Dockerfile.airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    container_name: airflow_init
    depends_on:
      - airflowdb
    environment: *airflow_environment
    entrypoint: /bin/bash
    command: -c 'airflow db init && airflow users create --username admin --password admin --firstname user --lastname test --role Admin --email admin@example.com && airflow db upgrade'
    restart: unless-stopped
    networks:
      - airflow_spark
  
  airflow_webserver:
    build:
      context: docker
      dockerfile: Dockerfile.airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    container_name: airflow_webserver
    depends_on:
      - airflow_init
    restart: always
    ports:
      - "8080:8080"
    networks:
      - airflow_spark
    volumes:
      - logs:/opt/airflow/logs
    environment: *airflow_environment
    command: webserver

  airflow_scheduler:
    build:
      context: docker
      dockerfile: Dockerfile.airflow
      args:
        AIRFLOW_BASE_IMAGE: *airflow_image
    container_name: airflow_scheduler
    depends_on:
      - airflow_init
    restart: always
    volumes:
      - logs:/opt/airflow/logs
      - ./airflow/dags:/opt/airflow/dags
      - ./spark-resources/src/:/opt/airflow/spark-apps/
    environment: *airflow_environment
    command: scheduler 
    networks:
      - airflow_spark

  spark-master:
    build: 
      context: docker
      dockerfile: Dockerfile.spark
    container_name: spark_master
    environment:
      - SPARK_MODE=master
      - SPARK_RPC_AUTHENTICATION_ENABLED=no 
      - SPARK_RPC_ENCRYPTION_ENABLED=no 
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8090:8080"
      - "7077:7077"
    networks:
      - airflow_spark
    volumes:
      - ./spark-resources/src/test.py:/opt/bitnami/spark/test.py
    depends_on:
      - airflow_init
    links:
      - airflow_scheduler
  
  spark-worker:
    build: 
      context: docker
      dockerfile: Dockerfile.spark
    container_name: spark_worker
    environment:
      - SPARK_MODE=worker
      - SPARK_MASTER_URL=spark://spark-master:7077
      - SPARK_WORKER_MEMORY=1G
      - SPARK_WORKER_CORES=1
      - SPARK_RPC_AUTHENTICATION_ENABLED=no 
      - SPARK_RPC_ENCRYPTION_ENABLED=no 
      - SPARK_LOCAL_STORAGE_ENCRYPTION_ENABLED=no
      - SPARK_SSL_ENABLED=no
    ports:
      - "8091:8081"
    networks:
      - airflow_spark
    volumes:
      - ./spark-resources/src/test.py:/opt/bitnami/spark/test.py
    depends_on:
      - spark-master
    links:
      - airflow_scheduler

# ------------- VOLUMES ------------------------------------
volumes:
  postgres-airflowdb-volume:
  logs:
